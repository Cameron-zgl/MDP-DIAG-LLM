{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650117b2",
   "metadata": {},
   "source": [
    "# RAG Pipeline with BioLLM (Pluggable Retrievers & Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd8155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:13<00:00,  2.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/gulizhu/MDP/biogpt_local'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "snapshot_download(repo_id=\"microsoft/biogpt\", local_dir=\"biogpt_local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba6d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_PATH = \"/home/gulizhu/MDP/biogpt_local\"   \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc38a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Config & Imports ===\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re, math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Paths to your data files\n",
    "CSV_PATH = Path(\"/home/gulizhu/MDP/combined_health_topics_with_source.csv\")\n",
    "TXT_PATH = Path(\"/home/gulizhu/MDP/textbook_pathology.txt\")\n",
    "XLSX_PATH = Path(\"/home/gulizhu/MDP/LLM Questions.xlsx\")\n",
    "\n",
    "# Model path (adjust to your BioLLM model)\n",
    "MODEL_PATH = \"/home/gulizhu/MDP/biogpt_local\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26b964",
   "metadata": {},
   "source": [
    "## 1. Load Data (CSV + TXT + Excel QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df55fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base size: 1286\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Common goods for health</td>\n",
       "      <td>health_topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Social determinants of health</td>\n",
       "      <td>health_topics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         context         source\n",
       "0        Common goods for health  health_topics\n",
       "1  Social determinants of health  health_topics"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Load CSV ---\n",
    "df_csv = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- Load TXT ---\n",
    "with open(TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    txt_content = f.read()\n",
    "df_txt = pd.DataFrame([{\"context\": txt_content, \"source\": \"textbook_pathology\"}])\n",
    "\n",
    "# --- Load Excel QA ---\n",
    "df_qa = pd.read_excel(XLSX_PATH)\n",
    "\n",
    "# Normalize QA columns (expect 'question', 'answer' at least)\n",
    "df_qa = df_qa.rename(columns={c: c.lower() for c in df_qa.columns})\n",
    "if \"question\" not in df_qa.columns:\n",
    "    raise ValueError(\"Excel QA file must contain a 'question' column\")\n",
    "\n",
    "# --- Combine knowledge sources ---\n",
    "if \"context\" not in df_csv.columns:\n",
    "    # assume one column holds text (choose first non-id column)\n",
    "    text_col = [c for c in df_csv.columns if c not in [\"id\",\"source\"]][0]\n",
    "    df_csv = df_csv.rename(columns={text_col: \"context\"})\n",
    "df_csv[\"source\"] = \"health_topics\"\n",
    "\n",
    "docs_df = pd.concat([df_csv[[\"context\",\"source\"]], df_txt], ignore_index=True)\n",
    "print(\"Knowledge base size:\", len(docs_df))\n",
    "docs_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0855218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question'], dtype='object')\n",
      "                                            question\n",
      "0  What is the role of a pathologist in cancer di...\n",
      "1  Which biomarkers are key in the analysis of br...\n",
      "2  How does a pathologist prepare and analyze a t...\n",
      "3  What are key features that a pathologist looks...\n",
      "4  What is immunohistochemistry and how is it use...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_qa = pd.read_excel(\"LLM Questions.xlsx\")\n",
    "print(df_qa.columns)\n",
    "print(df_qa.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72bb70",
   "metadata": {},
   "source": [
    "## 2. Define Retrievers (TF-IDF, BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30d0cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TFIDFRetriever:\n",
    "    def __init__(self, docs: List[str]):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=50000)\n",
    "        self.doc_mat = self.vectorizer.fit_transform(docs)\n",
    "        self.docs = docs\n",
    "\n",
    "    def search(self, query: str, k=5):\n",
    "        q_vec = self.vectorizer.transform([query])\n",
    "        sims = cosine_similarity(q_vec, self.doc_mat)[0]\n",
    "        idxs = sims.argsort()[::-1][:k]\n",
    "        return [(int(i), float(sims[i])) for i in idxs]\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, docs: List[str], k1=1.5, b=0.75):\n",
    "        self.docs = docs\n",
    "        self.k1, self.b = k1, b\n",
    "        self.tokenizer = re.compile(r\"\\w+\").findall\n",
    "        self.tokenized = [self.tokenizer(d.lower()) for d in docs]\n",
    "        self.doc_lens = [len(t) for t in self.tokenized]\n",
    "        self.avgdl = sum(self.doc_lens)/max(1,len(self.doc_lens))\n",
    "        df = defaultdict(int)\n",
    "        for toks in self.tokenized:\n",
    "            for w in set(toks):\n",
    "                df[w]+=1\n",
    "        self.N = len(docs)\n",
    "        self.idf = {w: math.log(1+(self.N-c+0.5)/(c+0.5)) for w,c in df.items()}\n",
    "        self.tf = [Counter(toks) for toks in self.tokenized]\n",
    "\n",
    "    def _score(self, q_toks, idx):\n",
    "        score=0.0; dl=self.doc_lens[idx]; tf_d=self.tf[idx]\n",
    "        for w in q_toks:\n",
    "            if w not in self.idf: continue\n",
    "            idf=self.idf[w]; f=tf_d.get(w,0)\n",
    "            denom=f+self.k1*(1-self.b+self.b*dl/(self.avgdl or 1))\n",
    "            score+=idf*(f*(self.k1+1))/(denom or 1e-12)\n",
    "        return score\n",
    "\n",
    "    def search(self, query:str,k=5):\n",
    "        q_toks=self.tokenizer(query.lower())\n",
    "        scores=[(i,self._score(q_toks,i)) for i in range(self.N)]\n",
    "        scores.sort(key=lambda x:x[1], reverse=True)\n",
    "        return scores[:k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5873b7",
   "metadata": {},
   "source": [
    "## 3. BioLLM Backend (swappable with other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a14d0f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class BioLLMBackend:\n",
    "    def __init__(self, model, tokenizer, device=\"cuda\"):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def generate(self, messages: List[Message]) -> str:\n",
    "        query = next((m.content for m in messages[::-1] if m.role==\"user\"), \"\")\n",
    "        context = \"\\n\\n\".join(m.content for m in messages if m.role in (\"system\",\"tool\"))\n",
    "        prompt = f\"Question: {query}\\nContext: {context}\\nAnswer:\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_length=256)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a7334",
   "metadata": {},
   "source": [
    "## 4. RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c19db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, docs_df: pd.DataFrame, retriever=\"tfidf\", llm=None):\n",
    "        self.df = docs_df.reset_index(drop=True)\n",
    "        self.contexts = self.df[\"context\"].astype(str).tolist()\n",
    "        if retriever==\"tfidf\":\n",
    "            self.retriever = TFIDFRetriever(self.contexts)\n",
    "        else:\n",
    "            self.retriever = BM25Retriever(self.contexts)\n",
    "        self.llm = llm\n",
    "\n",
    "    def ask(self, query: str, k=3):\n",
    "        hits = self.retriever.search(query, k)\n",
    "        msgs=[Message(role=\"tool\", content=self.contexts[i]) for i,_ in hits]\n",
    "        msgs.append(Message(role=\"user\", content=query))\n",
    "        ans = self.llm.generate(msgs)\n",
    "        return {\"query\":query, \"answer\":ans, \"hits\":hits}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cb469",
   "metadata": {},
   "source": [
    "## 5. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcabca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "llm = BioLLMBackend(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba162a",
   "metadata": {},
   "source": [
    "## 6. Compare Outputs Across Retrievers / Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d701de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 766492, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m                 rows.append({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m:q,\u001b[33m\"\u001b[39m\u001b[33mretriever\u001b[39m\u001b[33m\"\u001b[39m:rname,\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m:lname,\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m:out[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(rows)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m results = \u001b[43mcompare_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_qa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrievers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtfidf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbm25\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllms\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbiollm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcompare_answers\u001b[39m\u001b[34m(df_qa, retrievers, llms, n)\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m lname, lbackend \u001b[38;5;129;01min\u001b[39;00m llms:\n\u001b[32m      8\u001b[39m             rag = SimpleRAG(docs_df, retriever=rname, llm=lbackend)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m             out = \u001b[43mrag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m             rows.append({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m:q,\u001b[33m\"\u001b[39m\u001b[33mretriever\u001b[39m\u001b[33m\"\u001b[39m:rname,\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m:lname,\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m:out[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(rows)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mSimpleRAG.ask\u001b[39m\u001b[34m(self, query, k)\u001b[39m\n\u001b[32m     13\u001b[39m msgs=[Message(role=\u001b[33m\"\u001b[39m\u001b[33mtool\u001b[39m\u001b[33m\"\u001b[39m, content=\u001b[38;5;28mself\u001b[39m.contexts[i]) \u001b[38;5;28;01mfor\u001b[39;00m i,_ \u001b[38;5;129;01min\u001b[39;00m hits]\n\u001b[32m     14\u001b[39m msgs.append(Message(role=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, content=query))\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m ans = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m:query, \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m:ans, \u001b[33m\"\u001b[39m\u001b[33mhits\u001b[39m\u001b[33m\"\u001b[39m:hits}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mBioLLMBackend.generate\u001b[39m\u001b[34m(self, messages)\u001b[39m\n\u001b[32m     16\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envguli/venv311/lib64/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envguli/venv311/lib64/python3.11/site-packages/transformers/generation/utils.py:2068\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2065\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._supports_num_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnum_logits_to_keep\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[32m   2066\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_logits_to_keep\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2068\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[32m   2071\u001b[39m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[32m   2072\u001b[39m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[32m   2073\u001b[39m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[32m   2074\u001b[39m \u001b[38;5;66;03m# TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\u001b[39;00m\n\u001b[32m   2075\u001b[39m cache_name = \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmamba\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m.lower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcache_params\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envguli/venv311/lib64/python3.11/site-packages/transformers/generation/utils.py:1383\u001b[39m, in \u001b[36mGenerationMixin._validate_generated_length\u001b[39m\u001b[34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length >= generation_config.max_length:\n\u001b[32m   1382\u001b[39m     input_ids_string = \u001b[33m\"\u001b[39m\u001b[33mdecoder_input_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1383\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1384\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but `max_length` is set to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1385\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config.max_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1386\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1389\u001b[39m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[32m   1390\u001b[39m min_length_error_suffix = (\n\u001b[32m   1391\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1392\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mincrease the maximum length.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1393\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Input length of input_ids is 766492, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def compare_answers(df_qa: pd.DataFrame, retrievers=[\"tfidf\",\"bm25\"], llms=[(\"biollm\", llm)], n=5):\n",
    "    sample = df_qa.sample(min(n, len(df_qa)), random_state=0)\n",
    "    rows=[]\n",
    "    for _,row in sample.iterrows():\n",
    "        q = str(row[\"question\"])\n",
    "        for rname in retrievers:\n",
    "            for lname, lbackend in llms:\n",
    "                rag = SimpleRAG(docs_df, retriever=rname, llm=lbackend)\n",
    "                out = rag.ask(q, k=3)\n",
    "                rows.append({\"question\":q,\"retriever\":rname,\"model\":lname,\"answer\":out[\"answer\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "results = compare_answers(df_qa, retrievers=[\"tfidf\",\"bm25\"], llms=[(\"biollm\", llm)], n=5)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22fd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ultrasound-3.11)",
   "language": "python",
   "name": "ultrasound311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
