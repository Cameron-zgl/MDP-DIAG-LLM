{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce13f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sk-9788b084799748b9ac49471f46225d8f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce859eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Load BioLLM weights & base deps ===\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_PATH = \"/home/gulizhu/MDP/biogpt_local\"   \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# === Config & Imports ===\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re, math, numpy as np\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a2278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textbook chunks: 4759\n",
      "Knowledge base size: 6044\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Common goods for health are population-based f...</td>\n",
       "      <td>WHO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The social determinants of health (SDH) are th...</td>\n",
       "      <td>WHO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context source\n",
       "0  Common goods for health are population-based f...    WHO\n",
       "1  The social determinants of health (SDH) are th...    WHO"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 2: Load sources & QA ===\n",
    "\n",
    "# Paths to your data files\n",
    "CSV_PATH = Path(\"/home/gulizhu/MDP/combined_health_topics_with_source.csv\")  \n",
    "TXT_PATH = Path(\"/home/gulizhu/MDP/textbook_pathology.txt\")                  \n",
    "XLSX_PATH = Path(\"/home/gulizhu/MDP/LLM Questions.xlsx\")                     \n",
    "\n",
    "# --- Load CSV (WHO topics) ---\n",
    "df_csv = pd.read_csv(CSV_PATH)\n",
    "df_csv = df_csv.rename(columns={\"text\": \"context\"})\n",
    "df_csv[\"source\"] = \"WHO\"\n",
    "\n",
    "# --- Load TXT (pathology textbook) and chunk ---\n",
    "with open(TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    txt_content = f.read()\n",
    "\n",
    "chunk_size = 800  # adjust as needed\n",
    "txt_chunks = [txt_content[i:i+chunk_size] for i in range(0, len(txt_content), chunk_size)]\n",
    "df_txt = pd.DataFrame([{\"context\": chunk, \"source\": \"textbook_pathology\"} for chunk in txt_chunks])\n",
    "\n",
    "print(\"Textbook chunks:\", len(df_txt))\n",
    "\n",
    "# --- Load Excel QA ---\n",
    "df_qa = pd.read_excel(XLSX_PATH)\n",
    "df_qa = df_qa.rename(columns={c: c.lower() for c in df_qa.columns})\n",
    "\n",
    "if \"question\" not in df_qa.columns:\n",
    "    if \"q\" in df_qa.columns:       df_qa = df_qa.rename(columns={\"q\": \"question\"})\n",
    "    elif \"prompt\" in df_qa.columns: df_qa = df_qa.rename(columns={\"prompt\": \"question\"})\n",
    "    elif \"ques\" in df_qa.columns:   df_qa = df_qa.rename(columns={\"ques\": \"question\"})\n",
    "if \"question\" not in df_qa.columns:\n",
    "    raise ValueError(\"Excel QA file must contain a 'question' column\")\n",
    "\n",
    "# --- Combine knowledge sources ---\n",
    "docs_df = pd.concat([df_csv[[\"context\",\"source\"]], df_txt], ignore_index=True)\n",
    "print(\"Knowledge base size:\", len(docs_df))\n",
    "display(docs_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78b47490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the role of a pathologist in cancer di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which biomarkers are key in the analysis of br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does a pathologist prepare and analyze a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are key features that a pathologist looks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is immunohistochemistry and how is it use...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question\n",
       "0  What is the role of a pathologist in cancer di...\n",
       "1  Which biomarkers are key in the analysis of br...\n",
       "2  How does a pathologist prepare and analyze a t...\n",
       "3  What are key features that a pathologist looks...\n",
       "4  What is immunohistochemistry and how is it use..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 3: Retrievers, Embeddings, RAG backend (your code, consolidated) ===\n",
    "\n",
    "# Quick peek of QA\n",
    "print(df_qa.columns)\n",
    "display(df_qa.head())\n",
    "\n",
    "class TFIDFRetriever:\n",
    "    def __init__(self, docs: List[str]):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=50000)\n",
    "        self.doc_mat = self.vectorizer.fit_transform(docs)\n",
    "        self.docs = docs\n",
    "    def search(self, query: str, k=5):\n",
    "        q_vec = self.vectorizer.transform([query])\n",
    "        sims = cosine_similarity(q_vec, self.doc_mat)[0]\n",
    "        idxs = sims.argsort()[::-1][:k]\n",
    "        return [(int(i), float(sims[i])) for i in idxs]\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, docs: List[str], k1=1.5, b=0.75):\n",
    "        self.docs = docs\n",
    "        self.k1, self.b = k1, b\n",
    "        self.tokenizer = re.compile(r\"\\w+\").findall\n",
    "        self.tokenized = [self.tokenizer(d.lower()) for d in docs]\n",
    "        self.doc_lens = [len(t) for t in self.tokenized]\n",
    "        self.avgdl = sum(self.doc_lens)/max(1,len(self.doc_lens))\n",
    "        df = defaultdict(int)\n",
    "        for toks in self.tokenized:\n",
    "            for w in set(toks):\n",
    "                df[w]+=1\n",
    "        self.N = len(docs)\n",
    "        self.idf = {w: math.log(1+(self.N-c+0.5)/(c+0.5)) for w,c in df.items()}\n",
    "        self.tf = [Counter(toks) for toks in self.tokenized]\n",
    "    def _score(self, q_toks, idx):\n",
    "        score=0.0; dl=self.doc_lens[idx]; tf_d=self.tf[idx]\n",
    "        for w in q_toks:\n",
    "            if w not in self.idf: continue\n",
    "            idf=self.idf[w]; f=tf_d.get(w,0)\n",
    "            denom=f+self.k1*(1-self.b+self.b*dl/(self.avgdl or 1))\n",
    "            score+=idf*(f*(self.k1+1))/(denom or 1e-12)\n",
    "        return score\n",
    "    def search(self, query:str,k=5):\n",
    "        q_toks=self.tokenizer(query.lower())\n",
    "        scores=[(i,self._score(q_toks,i)) for i in range(self.N)]\n",
    "        scores.sort(key=lambda x:x[1], reverse=True)\n",
    "        return scores[:k]\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class BioLLMBackend:\n",
    "    def __init__(self, model, tokenizer, device=None):\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    def generate(self, messages: List[Message]) -> str:\n",
    "        query = next((m.content for m in messages[::-1] if m.role == \"user\"), \"\")\n",
    "        context = \"\\n\\n\".join(m.content for m in messages if m.role in (\"system\", \"tool\"))\n",
    "        context = context[:2000]\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, max_new_tokens=256, do_sample=True, top_p=0.95, temperature=0.7\n",
    "            )\n",
    "        raw = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = raw.split(\"Answer:\")[-1].strip()\n",
    "        return answer\n",
    "\n",
    "# Embedding backends config\n",
    "EMBED_MODELS = [\n",
    "    (\"minilm\", \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    (\"bge-small\", \"BAAI/bge-small-en-v1.5\"),\n",
    "]\n",
    "\n",
    "class EmbeddingBackend:\n",
    "    def embed_texts(self, texts): raise NotImplementedError\n",
    "    def embed_query(self, text):  raise NotImplementedError\n",
    "\n",
    "class SentenceTransformersEmbedding(EmbeddingBackend):\n",
    "    def __init__(self, model_id: str, device: str = None):\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.model = SentenceTransformer(model_id, device=device)\n",
    "    def embed_texts(self, texts):\n",
    "        vecs = self.model.encode(texts, batch_size=64, show_progress_bar=False,\n",
    "                                 convert_to_numpy=True, normalize_embeddings=True)\n",
    "        return vecs\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_texts([text])[0]\n",
    "\n",
    "class HFMeanPoolingEmbedding(EmbeddingBackend):\n",
    "    def __init__(self, model_id: str, device: str = None):\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModel.from_pretrained(model_id).to(device)\n",
    "        self.device = device\n",
    "    def _mean_pool(self, outputs, attention_mask):\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "        summed = (last_hidden * mask).sum(1)\n",
    "        counts = mask.sum(1).clamp(min=1e-9)\n",
    "        return (summed / counts).detach().cpu().numpy()\n",
    "    def embed_texts(self, texts):\n",
    "        import torch, numpy as _np\n",
    "        all_vecs = []\n",
    "        bs = 16\n",
    "        for i in range(0, len(texts), bs):\n",
    "            batch = texts[i:i+bs]\n",
    "            enc = self.tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.model(**enc)\n",
    "            vecs = self._mean_pool(out, enc[\"attention_mask\"])\n",
    "            vecs = vecs / (_np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-9)\n",
    "            all_vecs.append(vecs)\n",
    "        return _np.vstack(all_vecs)\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_texts([text])[0]\n",
    "\n",
    "class EmbeddingRetriever:\n",
    "    def __init__(self, docs, backend: EmbeddingBackend):\n",
    "        self.docs = docs\n",
    "        self.backend = backend\n",
    "        self.doc_vecs = self.backend.embed_texts(docs)\n",
    "    def search(self, query: str, k=5):\n",
    "        q = self.backend.embed_query(query)\n",
    "        sims = (self.doc_vecs @ q)\n",
    "        idxs = np.argsort(-sims)[:k]\n",
    "        return [(int(i), float(sims[i])) for i in idxs]\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"lexical shortlist (TF-IDF/BM25) -> embedding re-rank\"\"\"\n",
    "    def __init__(self, docs, base: str, embed_backend: EmbeddingBackend, top_m: int = 50):\n",
    "        self.docs = docs\n",
    "        self.top_m = top_m\n",
    "        self.embed_backend = embed_backend\n",
    "        if base == \"tfidf\": self.base = TFIDFRetriever(docs)\n",
    "        elif base == \"bm25\": self.base = BM25Retriever(docs)\n",
    "        else: raise ValueError(\"base must be 'tfidf' or 'bm25'\")\n",
    "        self.doc_vecs = self.embed_backend.embed_texts(docs)\n",
    "    def search(self, query: str, k=5):\n",
    "        base_hits = self.base.search(query, k=self.top_m)\n",
    "        cand_idxs = [i for i,_ in base_hits]\n",
    "        q_vec = self.embed_backend.embed_query(query)\n",
    "        sims = (self.doc_vecs[cand_idxs] @ q_vec)\n",
    "        order = np.argsort(-sims)[:k]\n",
    "        return [(int(cand_idxs[i]), float(sims[i])) for i in order]\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, docs_df: pd.DataFrame, retriever=\"tfidf\", llm=None,\n",
    "                 embed_backend: EmbeddingBackend = None, hybrid_top_m: int = 50):\n",
    "        self.df = docs_df.reset_index(drop=True)\n",
    "        self.contexts = self.df[\"context\"].astype(str).tolist()\n",
    "        self.llm = llm\n",
    "        if retriever == \"tfidf\":\n",
    "            self.retriever = TFIDFRetriever(self.contexts); self.retriever_name=\"tfidf\"; self.embedding_name=\"-\"\n",
    "        elif retriever == \"bm25\":\n",
    "            self.retriever = BM25Retriever(self.contexts);  self.retriever_name=\"bm25\";  self.embedding_name=\"-\"\n",
    "        elif retriever == \"embed\":\n",
    "            if embed_backend is None: raise ValueError(\"embed_backend required for retriever='embed'\")\n",
    "            self.retriever = EmbeddingRetriever(self.contexts, embed_backend)\n",
    "            self.retriever_name=\"embed\"; self.embedding_name=str(embed_backend.__class__.__name__)\n",
    "        elif retriever in (\"hybrid_tfidf\", \"hybrid_bm25\"):\n",
    "            if embed_backend is None: raise ValueError(\"embed_backend required for hybrid\")\n",
    "            base = \"tfidf\" if retriever==\"hybrid_tfidf\" else \"bm25\"\n",
    "            self.retriever = HybridRetriever(self.contexts, base=base, embed_backend=embed_backend, top_m=hybrid_top_m)\n",
    "            self.retriever_name=retriever; self.embedding_name=str(embed_backend.__class__.__name__)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown retriever: {retriever}\")\n",
    "    def ask(self, query: str, k=3):\n",
    "        hits = self.retriever.search(query, k)\n",
    "        msgs=[Message(role=\"tool\", content=self.contexts[i][:2000]) for i,_ in hits]\n",
    "        msgs.append(Message(role=\"user\", content=query))\n",
    "        ans = self.llm.generate(msgs)\n",
    "        return {\"query\":query,\n",
    "                \"answer\":ans,\n",
    "                \"hits\":hits,\n",
    "                \"context\":\" \".join(self.contexts[i][:500] for i,_ in hits)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e31fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>retriever</th>\n",
       "      <th>model</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is pathology used in diagnosing soft tissu...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>biollm</td>\n",
       "      <td>We review the literature on the clinical, hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is pathology used in diagnosing soft tissu...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>biollm</td>\n",
       "      <td>An update of the classification of soft tissue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the importance of margins in pathology...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>biollm</td>\n",
       "      <td>\"For Wilms tumor, margins should be at least 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the importance of margins in pathology...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>biollm</td>\n",
       "      <td>What is the significance of margins after brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Describe fluorescence in situ hybridization (F...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>biollm</td>\n",
       "      <td>• FISH is a molecular technique that provides ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Describe fluorescence in situ hybridization (F...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>biollm</td>\n",
       "      <td>a primer: What is the role of fluorescence in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question retriever   model  \\\n",
       "0  How is pathology used in diagnosing soft tissu...     tfidf  biollm   \n",
       "1  How is pathology used in diagnosing soft tissu...      bm25  biollm   \n",
       "2  What is the importance of margins in pathology...     tfidf  biollm   \n",
       "3  What is the importance of margins in pathology...      bm25  biollm   \n",
       "4  Describe fluorescence in situ hybridization (F...     tfidf  biollm   \n",
       "5  Describe fluorescence in situ hybridization (F...      bm25  biollm   \n",
       "\n",
       "                                              answer  \n",
       "0  We review the literature on the clinical, hist...  \n",
       "1  An update of the classification of soft tissue...  \n",
       "2  \"For Wilms tumor, margins should be at least 2...  \n",
       "3  What is the significance of margins after brea...  \n",
       "4  • FISH is a molecular technique that provides ...  \n",
       "5  a primer: What is the role of fluorescence in ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 4: Instantiate LLM backend & quick sanity (optional) ===\n",
    "llm = BioLLMBackend(model, tokenizer)\n",
    "\n",
    "def compare_answers(df_qa: pd.DataFrame, retrievers=[\"tfidf\",\"bm25\"],\n",
    "                    llms=[(\"biollm\", None)], n=5):\n",
    "    if llms[0][1] is None:\n",
    "        llms = [(\"biollm\", llm)]\n",
    "    sample = df_qa.sample(min(n, len(df_qa)), random_state=0)\n",
    "    rows=[]\n",
    "    for _,row in sample.iterrows():\n",
    "        q = str(row[\"question\"])\n",
    "        for rname in retrievers:\n",
    "            rag = SimpleRAG(docs_df, retriever=rname, llm=llm)\n",
    "            out = rag.ask(q, k=3)\n",
    "            rows.append({\"question\":q,\"retriever\":rname,\"model\":\"biollm\",\"answer\":out[\"answer\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "_ = compare_answers(df_qa, retrievers=[\"tfidf\",\"bm25\"], llms=[(\"biollm\", llm)], n=3)\n",
    "display(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c76bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coverage rows = 458\n",
      "faith rows    = 919\n",
      "chunks        = 139943\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Load two-dimension benchmark files ===\n",
    "BENCH_DIR = Path(\"/home/gulizhu/MDP/benchmark_data/coverage_faithfulness\")  \n",
    "COVERAGE_CSV = BENCH_DIR / \"coverage_dataset.csv\"\n",
    "FAITH_CSV    = BENCH_DIR / \"faithfulness_dataset.csv\"\n",
    "CHUNK_INDEX  = BENCH_DIR / \"chunk_index.csv\"\n",
    "\n",
    "assert COVERAGE_CSV.exists() and FAITH_CSV.exists() and CHUNK_INDEX.exists(), \"benchmark files missing\"\n",
    "\n",
    "coverage_df = pd.read_csv(COVERAGE_CSV)  \n",
    "faith_df    = pd.read_csv(FAITH_CSV)     \n",
    "chunk_idx   = pd.read_csv(CHUNK_INDEX)    \n",
    "\n",
    "print(\"coverage rows =\", len(coverage_df))\n",
    "print(\"faith rows    =\", len(faith_df))\n",
    "print(\"chunks        =\", len(chunk_idx))\n",
    "\n",
    "chunk_text = dict(zip(chunk_idx[\"chunk_id\"], chunk_idx[\"text\"]))\n",
    "chunk_doc  = dict(zip(chunk_idx[\"chunk_id\"], chunk_idx[\"doc_id\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04156c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retriever</th>\n",
       "      <th>embedding</th>\n",
       "      <th>K</th>\n",
       "      <th>Doc-Hit@K</th>\n",
       "      <th>Chunk-Hit@K</th>\n",
       "      <th>ContextRecall@K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bm25</td>\n",
       "      <td>-</td>\n",
       "      <td>10</td>\n",
       "      <td>0.358079</td>\n",
       "      <td>0.028384</td>\n",
       "      <td>0.503284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hybrid_tfidf</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.299127</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.503720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>10</td>\n",
       "      <td>0.296943</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.475534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hybrid_bm25</td>\n",
       "      <td>bge-small-en-v1.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.294760</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.535352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>embed</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.229258</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.536444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embed</td>\n",
       "      <td>bge-small-en-v1.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.216157</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.553757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      retriever          embedding   K  Doc-Hit@K  Chunk-Hit@K  \\\n",
       "1          bm25                  -  10   0.358079     0.028384   \n",
       "4  hybrid_tfidf   all-MiniLM-L6-v2  10   0.299127     0.034934   \n",
       "0         tfidf                  -  10   0.296943     0.034934   \n",
       "5   hybrid_bm25  bge-small-en-v1.5  10   0.294760     0.034934   \n",
       "2         embed   all-MiniLM-L6-v2  10   0.229258     0.034934   \n",
       "3         embed  bge-small-en-v1.5  10   0.216157     0.034934   \n",
       "\n",
       "   ContextRecall@K  \n",
       "1         0.503284  \n",
       "4         0.503720  \n",
       "0         0.475534  \n",
       "5         0.535352  \n",
       "2         0.536444  \n",
       "3         0.553757  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 6 (FIXED): Coverage evaluation that USES each retriever's own top-K ===\n",
    "import hashlib\n",
    "from functools import lru_cache\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", str(s)).strip().lower()\n",
    "    return s\n",
    "\n",
    "def _hash_text(s: str) -> str:\n",
    "    return hashlib.sha1(_norm_text(s).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "_CHUNK_TFIDF = {}\n",
    "_CHUNK_TFIDF[\"vec\"] = TfidfVectorizer(max_features=120000, ngram_range=(1,2))\n",
    "_CHUNK_TFIDF[\"X\"]   = _CHUNK_TFIDF[\"vec\"].fit_transform(chunk_idx[\"text\"].astype(str).tolist())\n",
    "_CHUNK_TFIDF[\"ids\"] = chunk_idx[\"chunk_id\"].astype(str).tolist()\n",
    "_CHUNK_ID_POS = {cid: i for i, cid in enumerate(_CHUNK_TFIDF[\"ids\"])}\n",
    "_chunk_hash_to_id = {}\n",
    "for cid, ctext in zip(chunk_idx[\"chunk_id\"].astype(str), chunk_idx[\"text\"].astype(str)):\n",
    "    _chunk_hash_to_id[_hash_text(ctext)] = cid\n",
    "\n",
    "_DOCIDX_TO_CHUNKID = {}\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def _map_doc_text_to_chunk_id(doc_text_norm: str) -> str:\n",
    "    h = hashlib.sha1(doc_text_norm.encode(\"utf-8\")).hexdigest()\n",
    "    if h in _chunk_hash_to_id:\n",
    "        return _chunk_hash_to_id[h]\n",
    "    qv = _CHUNK_TFIDF[\"vec\"].transform([doc_text_norm])\n",
    "    sim = cosine_similarity(qv, _CHUNK_TFIDF[\"X\"])[0]\n",
    "    j = int(np.argmax(sim))\n",
    "    return _CHUNK_TFIDF[\"ids\"][j]\n",
    "\n",
    "def map_doc_index_to_chunk_id(doc_idx: int) -> str:\n",
    "\n",
    "    if doc_idx in _DOCIDX_TO_CHUNKID:\n",
    "        return _DOCIDX_TO_CHUNKID[doc_idx]\n",
    "    raw = str(docs_df.iloc[doc_idx][\"context\"])\n",
    "    cid = _map_doc_text_to_chunk_id(_norm_text(raw))\n",
    "    _DOCIDX_TO_CHUNKID[doc_idx] = cid\n",
    "    return cid\n",
    "\n",
    "def _tokset(s: str):\n",
    "    return {t.lower() for t in re.findall(r\"\\b\\w+\\b\", str(s)) if len(t)>3}\n",
    "\n",
    "def build_embed_backend(model_id: str, device=None):\n",
    "    try:\n",
    "        return SentenceTransformersEmbedding(model_id, device=device)\n",
    "    except Exception:\n",
    "        return HFMeanPoolingEmbedding(model_id, device=device)\n",
    "\n",
    "RETRIEVER_MATRIX = [\n",
    "    (\"tfidf\",        None),                                        \n",
    "    (\"bm25\",         None),                                        \n",
    "    (\"embed\",        \"sentence-transformers/all-MiniLM-L6-v2\"),    \n",
    "    (\"embed\",        \"BAAI/bge-small-en-v1.5\"),\n",
    "    (\"hybrid_tfidf\", \"sentence-transformers/all-MiniLM-L6-v2\"),    \n",
    "    (\"hybrid_bm25\",  \"BAAI/bge-small-en-v1.5\"),                    \n",
    "]\n",
    "\n",
    "K = 30  \n",
    "\n",
    "def eval_coverage_for_combo(retriever_name: str, embed_model_id: str|None):\n",
    "    if embed_model_id is None:\n",
    "        rag = SimpleRAG(docs_df, retriever=retriever_name, llm=llm)\n",
    "        embed_short = \"-\"\n",
    "    else:\n",
    "        be = build_embed_backend(embed_model_id)\n",
    "        rag = SimpleRAG(docs_df, retriever=retriever_name, llm=llm, embed_backend=be)\n",
    "        embed_short = embed_model_id.split(\"/\")[-1]\n",
    "\n",
    "    rows = []\n",
    "    for _, r in coverage_df.iterrows():\n",
    "        qid, q, ans = r[\"qid\"], str(r[\"question\"]), str(r[\"answer\"])\n",
    "        gt_doc, gt_chunk = str(r[\"gt_doc_id\"]), str(r[\"gt_chunk_id\"])\n",
    "\n",
    "        hits = rag.retriever.search(q, k=K)              # [(doc_idx_in_docs_df, score)]\n",
    "        doc_indices = [i for (i, _) in hits]\n",
    "\n",
    "        top_chunk_ids = [map_doc_index_to_chunk_id(i) for i in doc_indices]\n",
    "\n",
    "        hit_doc   = int(any(str(cid).startswith(gt_doc) for cid in top_chunk_ids))\n",
    "        hit_chunk = int(gt_chunk in top_chunk_ids)\n",
    "\n",
    "        ctx = \" \\n\\n\".join([chunk_text.get(cid, \"\") for cid in top_chunk_ids])\n",
    "        A = _tokset(ans); C = _tokset(ctx)\n",
    "        ctx_recall = len(A & C) / (len(A) + 1e-9)\n",
    "\n",
    "        rows.append({\n",
    "            \"qid\": qid,\n",
    "            \"hit_doc@K\": hit_doc,\n",
    "            \"hit_chunk@K\": hit_chunk,\n",
    "            \"context_recall@K\": ctx_recall\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return {\n",
    "        \"retriever\": retriever_name,\n",
    "        \"embedding\": embed_short,\n",
    "        \"K\": K,\n",
    "        \"Doc-Hit@K\": float(df[\"hit_doc@K\"].mean()),\n",
    "        \"Chunk-Hit@K\": float(df[\"hit_chunk@K\"].mean()),\n",
    "        \"ContextRecall@K\": float(df[\"context_recall@K\"].mean()),\n",
    "        \"detail\": df\n",
    "    }\n",
    "\n",
    "coverage_summary = []\n",
    "coverage_details = {}\n",
    "\n",
    "for retriever_name, embed_model_id in RETRIEVER_MATRIX:\n",
    "    res = eval_coverage_for_combo(retriever_name, embed_model_id)\n",
    "    coverage_summary.append({k: res[k] for k in [\"retriever\",\"embedding\",\"K\",\"Doc-Hit@K\",\"Chunk-Hit@K\",\"ContextRecall@K\"]})\n",
    "    key = f\"{res['retriever']}__{res['embedding']}\"\n",
    "    coverage_details[key] = res[\"detail\"]\n",
    "\n",
    "coverage_table = pd.DataFrame(coverage_summary).sort_values(\n",
    "    [\"Doc-Hit@K\",\"Chunk-Hit@K\",\"ContextRecall@K\"], ascending=False\n",
    ")\n",
    "display(coverage_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "665db30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>retriever</th>\n",
       "      <th>embedding</th>\n",
       "      <th>answer</th>\n",
       "      <th>evidence_chunks</th>\n",
       "      <th>faithfulness_score_retrieved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q::b7b927d779</td>\n",
       "      <td>What is the purpose of the assistance mentioned?</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>In the following, we present a review of the m...</td>\n",
       "      <td>TEXTBOOK::0001::CH0052|TEXTBOOK::0007::CH0152|...</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q::a4aedf3d70</td>\n",
       "      <td>What does the acronym NIH stand for?</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>TEXTBOOK::0001::CH0052|WHO::3b019d8021::CH0002...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q::1a6c0caae5</td>\n",
       "      <td>What is the full name of the NIDCD?</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>The major etiological factor responsible for t...</td>\n",
       "      <td>TEXTBOOK::0001::CH0052|TEXTBOOK::0006::CH1693|...</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q::4d04fecd09</td>\n",
       "      <td>What is the full name of the organization abbr...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>What is the full name of the CDC?</td>\n",
       "      <td>TEXTBOOK::0003::CH0226|TEXTBOOK::0003::CH0223|...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q::be034ca29c</td>\n",
       "      <td>How long do these symptoms typically last?</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>What is the best treatment for them?</td>\n",
       "      <td>WHO::0363a8d07f::CH0000|WHO::acbab8c20b::CH000...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             qid                                           question retriever  \\\n",
       "0  Q::b7b927d779   What is the purpose of the assistance mentioned?     tfidf   \n",
       "1  Q::a4aedf3d70               What does the acronym NIH stand for?     tfidf   \n",
       "2  Q::1a6c0caae5                What is the full name of the NIDCD?     tfidf   \n",
       "3  Q::4d04fecd09  What is the full name of the organization abbr...     tfidf   \n",
       "4  Q::be034ca29c         How long do these symptoms typically last?     tfidf   \n",
       "\n",
       "  embedding                                             answer  \\\n",
       "0         -  In the following, we present a review of the m...   \n",
       "1         -                                                ...   \n",
       "2         -  The major etiological factor responsible for t...   \n",
       "3         -                  What is the full name of the CDC?   \n",
       "4         -               What is the best treatment for them?   \n",
       "\n",
       "                                     evidence_chunks  \\\n",
       "0  TEXTBOOK::0001::CH0052|TEXTBOOK::0007::CH0152|...   \n",
       "1  TEXTBOOK::0001::CH0052|WHO::3b019d8021::CH0002...   \n",
       "2  TEXTBOOK::0001::CH0052|TEXTBOOK::0006::CH1693|...   \n",
       "3  TEXTBOOK::0003::CH0226|TEXTBOOK::0003::CH0223|...   \n",
       "4  WHO::0363a8d07f::CH0000|WHO::acbab8c20b::CH000...   \n",
       "\n",
       "   faithfulness_score_retrieved  \n",
       "0                      0.157895  \n",
       "1                      0.000000  \n",
       "2                      0.444444  \n",
       "3                      0.333333  \n",
       "4                      0.500000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg faithfulness (to retrieved evidence) by combo:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retriever</th>\n",
       "      <th>embedding</th>\n",
       "      <th>faithfulness_score_retrieved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bm25</td>\n",
       "      <td>-</td>\n",
       "      <td>0.475341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hybrid_bm25</td>\n",
       "      <td>bge-small-en-v1.5</td>\n",
       "      <td>0.457950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>embed</td>\n",
       "      <td>bge-small-en-v1.5</td>\n",
       "      <td>0.425229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>embed</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>0.408902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>0.394703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hybrid_tfidf</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>0.392703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      retriever          embedding  faithfulness_score_retrieved\n",
       "0          bm25                  -                      0.475341\n",
       "3   hybrid_bm25  bge-small-en-v1.5                      0.457950\n",
       "2         embed  bge-small-en-v1.5                      0.425229\n",
       "1         embed   all-MiniLM-L6-v2                      0.408902\n",
       "5         tfidf                  -                      0.394703\n",
       "4  hybrid_tfidf   all-MiniLM-L6-v2                      0.392703"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 7a: Faithfulness to retrieved evidence mapped to chunk_index ===\n",
    "def token_recall(pred, evid):\n",
    "    A = {t.lower() for t in re.findall(r\"\\b\\w+\\b\", str(pred)) if len(t)>3}\n",
    "    E = {t.lower() for t in re.findall(r\"\\b\\w+\\b\", str(evid)) if len(t)>3}\n",
    "    return len(A & E) / (len(A) + 1e-9)\n",
    "\n",
    "def eval_faithfulness_retrieved_chunks(retriever_name: str, embed_model_id: str|None,\n",
    "                                       K_gen=3, N=None):\n",
    "    if embed_model_id is None:\n",
    "        rag = SimpleRAG(docs_df, retriever=retriever_name, llm=llm)\n",
    "        embed_short = \"-\"\n",
    "    else:\n",
    "        be = build_embed_backend(embed_model_id)\n",
    "        rag = SimpleRAG(docs_df, retriever=retriever_name, llm=llm, embed_backend=be)\n",
    "        embed_short = embed_model_id.split(\"/\")[-1]\n",
    "\n",
    "    gold = faith_df[faith_df[\"note\"].isna()].copy() \n",
    "    if N is not None and len(gold) > N:\n",
    "        gold = gold.sample(N, random_state=0)\n",
    "\n",
    "    rows=[]\n",
    "    for _, r in gold.iterrows():\n",
    "        qid, q = r[\"qid\"], str(r[\"question\"])\n",
    "        out = rag.ask(q, k=K_gen)                         \n",
    "        ans = out[\"answer\"]\n",
    "        doc_idxs = [i for i,_ in out[\"hits\"]]             \n",
    "        chunk_ids = [map_doc_index_to_chunk_id(i) for i in doc_idxs]\n",
    "        evid = \" \".join(chunk_text[cid][:500] for cid in chunk_ids)\n",
    "        score = token_recall(ans, evid)\n",
    "        rows.append({\n",
    "            \"qid\": qid, \"question\": q,\n",
    "            \"retriever\": retriever_name, \"embedding\": embed_short,\n",
    "            \"answer\": ans, \"evidence_chunks\": \"|\".join(chunk_ids),\n",
    "            \"faithfulness_score_retrieved\": score\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "faith_out_retr = []\n",
    "for retriever_name, embed_model_id in RETRIEVER_MATRIX:\n",
    "    df = eval_faithfulness_retrieved_chunks(retriever_name, embed_model_id,\n",
    "                                            K_gen=3, N=None)\n",
    "    faith_out_retr.append(df)\n",
    "\n",
    "faith_retrieved = pd.concat(faith_out_retr, ignore_index=True)\n",
    "display(faith_retrieved.head())\n",
    "print(\"Avg faithfulness (to retrieved evidence) by combo:\")\n",
    "display(faith_retrieved.groupby([\"retriever\",\"embedding\"])[\"faithfulness_score_retrieved\"]\n",
    "        .mean().reset_index().sort_values(\"faithfulness_score_retrieved\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d76114b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>retriever</th>\n",
       "      <th>embedding</th>\n",
       "      <th>answer</th>\n",
       "      <th>gold_evidence_chunk</th>\n",
       "      <th>faithfulness_score_gold</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q::b7b927d779</td>\n",
       "      <td>What is the purpose of the assistance mentioned?</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>Answer yes!</td>\n",
       "      <td>WHO::def5effffe::CH0075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q::a4aedf3d70</td>\n",
       "      <td>What does the acronym NIH stand for?</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>inactivity or inactivity, inactivity, low acti...</td>\n",
       "      <td>WHO::ba091c3aa0::CH0022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q::1a6c0caae5</td>\n",
       "      <td>What is the full name of the NIDCD?</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>We should learn about the nosology of NIDCD an...</td>\n",
       "      <td>WHO::ba091c3aa0::CH0022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q::4d04fecd09</td>\n",
       "      <td>What is the full name of the organization abbr...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>We have to recognize the clinical importance o...</td>\n",
       "      <td>WHO::c746a8289b::CH0048</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q::be034ca29c</td>\n",
       "      <td>How long do these symptoms typically last?</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>Do you have them?</td>\n",
       "      <td>WHO::5d456f490d::CH0041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             qid                                           question retriever  \\\n",
       "0  Q::b7b927d779   What is the purpose of the assistance mentioned?     tfidf   \n",
       "1  Q::a4aedf3d70               What does the acronym NIH stand for?     tfidf   \n",
       "2  Q::1a6c0caae5                What is the full name of the NIDCD?     tfidf   \n",
       "3  Q::4d04fecd09  What is the full name of the organization abbr...     tfidf   \n",
       "4  Q::be034ca29c         How long do these symptoms typically last?     tfidf   \n",
       "\n",
       "  embedding                                             answer  \\\n",
       "0         -                                        Answer yes!   \n",
       "1         -  inactivity or inactivity, inactivity, low acti...   \n",
       "2         -  We should learn about the nosology of NIDCD an...   \n",
       "3         -  We have to recognize the clinical importance o...   \n",
       "4         -                                  Do you have them?   \n",
       "\n",
       "       gold_evidence_chunk  faithfulness_score_gold  label  \n",
       "0  WHO::def5effffe::CH0075                 0.000000      1  \n",
       "1  WHO::ba091c3aa0::CH0022                 0.000000      1  \n",
       "2  WHO::ba091c3aa0::CH0022                 0.000000      1  \n",
       "3  WHO::c746a8289b::CH0048                 0.090909      1  \n",
       "4  WHO::5d456f490d::CH0041                 0.000000      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retriever</th>\n",
       "      <th>embedding</th>\n",
       "      <th>AUC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hybrid_bm25</td>\n",
       "      <td>bge-small-en-v1.5</td>\n",
       "      <td>0.476710</td>\n",
       "      <td>0.041215</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>-</td>\n",
       "      <td>0.471616</td>\n",
       "      <td>0.030369</td>\n",
       "      <td>0.050955</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embed</td>\n",
       "      <td>bge-small-en-v1.5</td>\n",
       "      <td>0.388646</td>\n",
       "      <td>0.041215</td>\n",
       "      <td>0.067511</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bm25</td>\n",
       "      <td>-</td>\n",
       "      <td>0.345706</td>\n",
       "      <td>0.032538</td>\n",
       "      <td>0.051064</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hybrid_tfidf</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>0.326419</td>\n",
       "      <td>0.043384</td>\n",
       "      <td>0.071579</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>embed</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>0.312955</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.046809</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      retriever          embedding       AUC       ACC        F1  threshold\n",
       "5   hybrid_bm25  bge-small-en-v1.5  0.476710  0.041215  0.071429        0.5\n",
       "0         tfidf                  -  0.471616  0.030369  0.050955        0.5\n",
       "3         embed  bge-small-en-v1.5  0.388646  0.041215  0.067511        0.5\n",
       "1          bm25                  -  0.345706  0.032538  0.051064        0.5\n",
       "4  hybrid_tfidf   all-MiniLM-L6-v2  0.326419  0.043384  0.071579        0.5\n",
       "2         embed   all-MiniLM-L6-v2  0.312955  0.028200  0.046809        0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 7b: Faithfulness to GOLD evidence (labels in faithfulness_dataset.csv) ===\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "def eval_faithfulness_gold_evidence(retriever_name: str, embed_model_id: str|None,\n",
    "                                    K_gen=3, N=None, threshold=0.5):\n",
    "    if embed_model_id is None:\n",
    "        rag = SimpleRAG(docs_df, retriever=retriever_name, llm=llm)\n",
    "        embed_short = \"-\"\n",
    "    else:\n",
    "        be = build_embed_backend(embed_model_id)\n",
    "        rag = SimpleRAG(docs_df, retriever=retriever_name, llm=llm, embed_backend=be)\n",
    "        embed_short = embed_model_id.split(\"/\")[-1]\n",
    "\n",
    "    gold = faith_df[faith_df[\"note\"].isna()].copy()  \n",
    "    if N is not None and len(gold) > N:\n",
    "        gold = gold.sample(N, random_state=0)\n",
    "\n",
    "    scores, labels, rows = [], [], []\n",
    "    for _, r in gold.iterrows():\n",
    "        qid, q = r[\"qid\"], str(r[\"question\"])\n",
    "        ans = rag.ask(q, k=K_gen)[\"answer\"]  \n",
    "        evid_cid = str(r[\"evidence_chunk_id\"])\n",
    "        evid = chunk_text.get(evid_cid, \"\")\n",
    "        s = token_recall(ans, evid)          \n",
    "        scores.append(float(s))\n",
    "        labels.append(int(r[\"label_faithful\"]))\n",
    "        rows.append({\n",
    "            \"qid\": qid, \"question\": q, \"retriever\": retriever_name,\n",
    "            \"embedding\": embed_short, \"answer\": ans,\n",
    "            \"gold_evidence_chunk\": evid_cid, \"faithfulness_score_gold\": float(s),\n",
    "            \"label\": int(r[\"label_faithful\"])\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    auc = roc_auc_score(labels, scores) if len(set(labels))>1 else float(\"nan\")\n",
    "    pred = [s>=threshold for s in scores]\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    f1  = f1_score(labels, pred)\n",
    "    summary = {\"retriever\": retriever_name, \"embedding\": embed_short,\n",
    "               \"AUC\": auc, \"ACC\": acc, \"F1\": f1, \"threshold\": threshold}\n",
    "    return df, summary\n",
    "\n",
    "gold_rows, gold_summ = [], []\n",
    "for retriever_name, embed_model_id in RETRIEVER_MATRIX:\n",
    "    df, summ = eval_faithfulness_gold_evidence(retriever_name, embed_model_id,\n",
    "                                               K_gen=3, N=None, threshold=0.5)\n",
    "    gold_rows.append(df); gold_summ.append(summ)\n",
    "\n",
    "faith_gold = pd.concat(gold_rows, ignore_index=True)\n",
    "faith_gold_summary = pd.DataFrame(gold_summ).sort_values(\"AUC\", ascending=False)\n",
    "\n",
    "display(faith_gold.head())\n",
    "display(faith_gold_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from transformers import pipeline as hf_pipeline\n",
    "    DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "    _nli = hf_pipeline(\"text-classification\", model=\"microsoft/deberta-v3-base-mnli\", device=DEVICE)\n",
    "    def nli_entail_prob(answer: str, evidence: str) -> float:\n",
    "        inp = f\"premise: {evidence}\\nhypothesis: {answer}\"\n",
    "        out = _nli(inp, truncation=True)[0]\n",
    "        return float(out[\"score\"]) if \"ENTAIL\" in out[\"label\"].upper() else 1.0 - float(out[\"score\"])\n",
    "    USE_NLI = True\n",
    "    print(\"NLI judge loaded.\")\n",
    "except Exception as e:\n",
    "    print(\"NLI judge not available:\", repr(e))\n",
    "    USE_NLI = False\n",
    "\n",
    "if USE_NLI:\n",
    "    rows=[]\n",
    "    for _, r in faith_result.iterrows():\n",
    "        score = nli_entail_prob(r[\"answer\"], r.get(\"evidence\", r.get(\"context\",\"\")))\n",
    "        rows.append(score)\n",
    "    faith_result[\"faithfulness_nli\"] = rows\n",
    "    display(faith_result.head())\n",
    "    print(\"Avg NLI faithfulness by combo:\")\n",
    "    display(faith_result.groupby([\"retriever\",\"embedding\"])[\"faithfulness_nli\"].mean().reset_index().sort_values(\"faithfulness_nli\", ascending=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ultrasound-3.11)",
   "language": "python",
   "name": "ultrasound311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
