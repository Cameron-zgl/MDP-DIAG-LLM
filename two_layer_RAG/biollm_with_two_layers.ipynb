{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes transformers accelerate \\\n",
        "    langchain langchain-community langchain-huggingface \\\n",
        "    chromadb openpyxl requests beautifulsoup4 PyPDF2"
      ],
      "metadata": {
        "id": "QDV4ez7ly64L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import re\n",
        "\n",
        "with open(\"bio_urls.txt\", \"r\") as f:\n",
        "    cleaned_urls = [re.sub(r'(?<!:)//+', '/', line.strip()) for line in f if line.strip()]\n",
        "\n",
        "with open(\"bio_urls_cleaned.txt\", \"w\") as f:\n",
        "    for url in cleaned_urls:\n",
        "        f.write(url + \"\\n\")\n",
        "'''"
      ],
      "metadata": {
        "id": "_EmuUXlb0Fn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import requests\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from io import BytesIO\n",
        "\n",
        "def fetch_and_load_pdf(url: str):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "    }\n",
        "    try:\n",
        "        print(f\"ðŸ”— Trying URL: {url}\")\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Check for correct content type (application/pdf)\n",
        "        if \"application/pdf\" not in response.headers.get(\"Content-Type\", \"\"):\n",
        "            print(f\"âŒ Not a PDF (content-type was: {response.headers.get('Content-Type')})\")\n",
        "            return None\n",
        "\n",
        "        loader = PyPDFLoader(BytesIO(response.content))\n",
        "        return loader.load()\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Exception while fetching PDF from {url}: {e}\")\n",
        "        return None\n",
        "'''"
      ],
      "metadata": {
        "id": "T2W9Wvt62t0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import openpyxl\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# ===== MODEL SETUP =====\n",
        "model_name = \"aaditya/Llama3-OpenBioLLM-8B\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_skip_modules=None\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "def run_biollm(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "                  **inputs,\n",
        "                  max_new_tokens=256,\n",
        "                  do_sample=True,\n",
        "                  pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n",
        "\n",
        "# ===== GLOBAL SETUP =====\n",
        "curRow = 2\n",
        "columnHeaders = [\"MODEL\", \"Prompt\", \"Question\", \"Context\", \"ContextSource\", \"Answer\"]\n",
        "headers = {header: idx + 1 for idx, header in enumerate(columnHeaders)}\n",
        "\n",
        "# ===== UTILITIES =====\n",
        "def write_cell(sheet, row, col, value):\n",
        "    sheet.cell(row=row, column=col).value = value\n",
        "\n",
        "def add_row(sheet, row_data):\n",
        "    global curRow\n",
        "    for key in columnHeaders:\n",
        "        sheet.cell(row=curRow, column=headers[key]).value = row_data.get(key, \"\")\n",
        "    curRow += 1\n",
        "\n",
        "def load_pdf_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(\"temp.pdf\", \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        reader = PdfReader(\"temp.pdf\")\n",
        "        return \"\".join([page.extract_text() or \"\" for page in reader.pages])\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load PDF from {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def scrape_website(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        return \"\\n\".join([p.get_text() for p in soup.find_all(\"p\")])\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "'''\n",
        "def load_data():\n",
        "    pdf_urls = [\n",
        "        \"https://www.biorxiv.org/content/10.1101/2020.07.28.224253v1.full.pdf\"\n",
        "    ]\n",
        "\n",
        "    website_urls = [\n",
        "        \"https://en.wikipedia.org/wiki/Pathology\",\n",
        "        \"https://www.mcgill.ca/pathology/about/definition\"\n",
        "    ]\n",
        "\n",
        "    documents_with_metadata = []\n",
        "\n",
        "    for url in pdf_urls:\n",
        "        data = load_pdf_from_url(url)\n",
        "        if data:\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "            splits = text_splitter.split_text(data)\n",
        "            for split in splits:\n",
        "                if split.strip():\n",
        "                    documents_with_metadata.append(Document(page_content=split, metadata={\"source\": url}))\n",
        "\n",
        "    for url in website_urls:\n",
        "        data = scrape_website(url)\n",
        "        if data:\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "            splits = text_splitter.split_text(data)\n",
        "            for split in splits:\n",
        "                if split.strip():\n",
        "                    documents_with_metadata.append(Document(page_content=split, metadata={\"source\": url}))\n",
        "\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
        "    vectorstore = Chroma(\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"chromadb\"\n",
        "    )\n",
        "\n",
        "    vectorstore.add_documents(documents_with_metadata)\n",
        "    vectorstore.persist()\n",
        "\n",
        "    return vectorstore\n",
        "'''\n",
        "\n",
        "def load_data():\n",
        "    documents_with_metadata = []\n",
        "\n",
        "    # ========== Load from combined_health_topics_with_source.csv ==========\n",
        "    try:\n",
        "        df = pd.read_csv(\"combined_health_topics_with_source.csv\")\n",
        "        for _, row in df.iterrows():\n",
        "            text = str(row.get(\"text\", \"\")).strip()\n",
        "            source = str(row.get(\"source\", \"\")).strip()\n",
        "            if text:\n",
        "                chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0).split_text(text)\n",
        "                for chunk in chunks:\n",
        "                    if chunk.strip():\n",
        "                        documents_with_metadata.append(Document(page_content=chunk, metadata={\"source\": source}))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\")\n",
        "\n",
        "    # ========== Load from textbook_pathology.txt ==========\n",
        "    try:\n",
        "        with open(\"textbook_pathology.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            textbook_content = f.read()\n",
        "            chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0).split_text(textbook_content)\n",
        "            for chunk in chunks:\n",
        "                if chunk.strip():\n",
        "                    documents_with_metadata.append(Document(page_content=chunk, metadata={\"source\": \"Textbook of Pathology\"}))\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading textbook: {e}\")\n",
        "\n",
        "    # ===== Load BioRxiv PDF links from file =====\n",
        "    try:\n",
        "        with open(\"Usable Documents From biorxiv.org.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            urls = [line.strip().split(\"|\")[0].strip() for line in f if line.strip()]\n",
        "        for url in urls:\n",
        "            print(f\"Loading PDF from: {url}\")\n",
        "            text = load_pdf_from_url(url)\n",
        "            if text:\n",
        "                chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0).split_text(text)\n",
        "                for chunk in chunks:\n",
        "                    if chunk.strip():\n",
        "                        documents_with_metadata.append(Document(page_content=chunk, metadata={\"source\": url}))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading BioRxiv URLs: {e}\")\n",
        "\n",
        "    # ========== Embed and store in Chroma ==========\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
        "    vectorstore = Chroma(\n",
        "        embedding_function=embedding_model,\n",
        "        persist_directory=\"chromadb\"\n",
        "    )\n",
        "    # NEW: Batch insertion\n",
        "    batch_size = 5000  # You can reduce this further if needed\n",
        "    for i in range(0, len(documents_with_metadata), batch_size):\n",
        "        batch = documents_with_metadata[i:i+batch_size]\n",
        "        vectorstore.add_documents(batch)\n",
        "    vectorstore.persist()\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "# ===== TWO-LAYER RAG GENERATION =====\n",
        "def query_two_layer_rag(vectorstore, question):\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    context_docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    # First Layer: Summarize each chunk\n",
        "    layer1_summaries = []\n",
        "    for doc in context_docs:\n",
        "        chunk = doc.page_content\n",
        "        prompt_layer1 = f\"\"\"\n",
        "        Summarize the following text briefly regarding this question: '{question}'.\n",
        "        If irrelevant, respond with \"Not relevant.\"\n",
        "\n",
        "        Text: {chunk}\n",
        "        \"\"\"\n",
        "        summary = run_biollm(prompt_layer1)\n",
        "        if summary.lower() != \"not relevant\":\n",
        "            layer1_summaries.append(summary)\n",
        "\n",
        "    # Second Layer: Combine and summarize\n",
        "    combined_summaries = \"\\n\\n\".join(layer1_summaries)\n",
        "    prompt_layer2 = f\"\"\"\n",
        "    Using these summaries:\n",
        "\n",
        "    {combined_summaries}\n",
        "\n",
        "    Generate a concise, final answer to the question '{question}'.\n",
        "    \"\"\"\n",
        "    final_answer = run_biollm(prompt_layer2)\n",
        "\n",
        "    return {\n",
        "        \"MODEL\": \"Llama3-OpenBioLLM-8B-Two-Layer-RAG\",\n",
        "        \"Question\": question,\n",
        "        \"Prompt\": prompt_layer2,\n",
        "        \"Context\": combined_summaries,\n",
        "        \"Answer\": final_answer\n",
        "    }\n",
        "\n",
        "# ===== MAIN PIPELINE =====\n",
        "def main():\n",
        "    #wbq = openpyxl.load_workbook(\"LLM Questions.xlsx\")\n",
        "    wbq = openpyxl.load_workbook(\"100questions.xlsx\")\n",
        "    sheet_q = wbq.active\n",
        "    questions = [sheet_q.cell(row=i, column=1).value for i in range(2, sheet_q.max_row + 1) if sheet_q.cell(row=i, column=1).value]\n",
        "\n",
        "    vectorstore = load_data()\n",
        "    wb = openpyxl.Workbook()\n",
        "    sheet = wb.active\n",
        "    for i, header in enumerate(columnHeaders):\n",
        "        write_cell(sheet, 1, i + 1, header)\n",
        "\n",
        "    for q in questions:\n",
        "        row_data = query_two_layer_rag(vectorstore, q)\n",
        "        add_row(sheet, row_data)\n",
        "\n",
        "    wb.save(\"sample.xlsx\")\n",
        "    print(\"Results saved to sample.xlsx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "dXulUlAXy4n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "351VTu4FDMzC"
      }
    }
  ]
}