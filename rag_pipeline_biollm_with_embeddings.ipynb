{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650117b2",
   "metadata": {},
   "source": [
    "# RAG Pipeline with BioLLM (Pluggable Retrievers & Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd8155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import snapshot_download\n",
    "#snapshot_download(repo_id=\"microsoft/biogpt\", local_dir=\"biogpt_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba6d3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gulizhu/envguli/venv311/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_PATH = \"/home/gulizhu/MDP/biogpt_local\"   \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc38a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Config & Imports ===\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re, math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Paths to your data files\n",
    "CSV_PATH = Path(\"/home/gulizhu/MDP/combined_health_topics_with_source.csv\")\n",
    "TXT_PATH = Path(\"/home/gulizhu/MDP/textbook_pathology.txt\")\n",
    "XLSX_PATH = Path(\"/home/gulizhu/MDP/LLM Questions.xlsx\")\n",
    "\n",
    "# Model path (adjust to your BioLLM model)\n",
    "MODEL_PATH = \"/home/gulizhu/MDP/biogpt_local\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26b964",
   "metadata": {},
   "source": [
    "## 1. Load Data (CSV + TXT + Excel QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae308ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textbook chunks: 4759\n",
      "Knowledge base size: 6044\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Common goods for health are population-based f...</td>\n",
       "      <td>WHO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The social determinants of health (SDH) are th...</td>\n",
       "      <td>WHO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context source\n",
       "0  Common goods for health are population-based f...    WHO\n",
       "1  The social determinants of health (SDH) are th...    WHO"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Load CSV (WHO topics) ---\n",
    "df_csv = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Use 'text' column as context\n",
    "df_csv = df_csv.rename(columns={\"text\": \"context\"})\n",
    "df_csv[\"source\"] = \"WHO\"\n",
    "\n",
    "# --- Load TXT (pathology textbook) and chunk ---\n",
    "with open(TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    txt_content = f.read()\n",
    "\n",
    "chunk_size = 800  # adjust as needed\n",
    "txt_chunks = [txt_content[i:i+chunk_size] for i in range(0, len(txt_content), chunk_size)]\n",
    "df_txt = pd.DataFrame([{\"context\": chunk, \"source\": \"textbook_pathology\"} for chunk in txt_chunks])\n",
    "\n",
    "print(\"Textbook chunks:\", len(df_txt))\n",
    "\n",
    "# --- Load Excel QA ---\n",
    "df_qa = pd.read_excel(XLSX_PATH)\n",
    "df_qa = df_qa.rename(columns={c: c.lower() for c in df_qa.columns})\n",
    "\n",
    "# normalize question column\n",
    "if \"question\" not in df_qa.columns:\n",
    "    if \"q\" in df_qa.columns:\n",
    "        df_qa = df_qa.rename(columns={\"q\": \"question\"})\n",
    "    elif \"prompt\" in df_qa.columns:\n",
    "        df_qa = df_qa.rename(columns={\"prompt\": \"question\"})\n",
    "    elif \"ques\" in df_qa.columns:\n",
    "        df_qa = df_qa.rename(columns={\"ques\": \"question\"})\n",
    "if \"question\" not in df_qa.columns:\n",
    "    raise ValueError(\"Excel QA file must contain a question-like column\")\n",
    "\n",
    "# --- Combine knowledge sources ---\n",
    "docs_df = pd.concat([df_csv[[\"context\",\"source\"]], df_txt], ignore_index=True)\n",
    "print(\"Knowledge base size:\", len(docs_df))\n",
    "docs_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0855218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question'], dtype='object')\n",
      "                                            question\n",
      "0  What is the role of a pathologist in cancer di...\n",
      "1  Which biomarkers are key in the analysis of br...\n",
      "2  How does a pathologist prepare and analyze a t...\n",
      "3  What are key features that a pathologist looks...\n",
      "4  What is immunohistochemistry and how is it use...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_qa = pd.read_excel(\"LLM Questions.xlsx\")\n",
    "print(df_qa.columns)\n",
    "print(df_qa.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72bb70",
   "metadata": {},
   "source": [
    "## 2. Define Retrievers (TF-IDF, BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d0cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TFIDFRetriever:\n",
    "    def __init__(self, docs: List[str]):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=50000)\n",
    "        self.doc_mat = self.vectorizer.fit_transform(docs)\n",
    "        self.docs = docs\n",
    "\n",
    "    def search(self, query: str, k=5):\n",
    "        q_vec = self.vectorizer.transform([query])\n",
    "        sims = cosine_similarity(q_vec, self.doc_mat)[0]\n",
    "        idxs = sims.argsort()[::-1][:k]\n",
    "        return [(int(i), float(sims[i])) for i in idxs]\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, docs: List[str], k1=1.5, b=0.75):\n",
    "        self.docs = docs\n",
    "        self.k1, self.b = k1, b\n",
    "        self.tokenizer = re.compile(r\"\\w+\").findall\n",
    "        self.tokenized = [self.tokenizer(d.lower()) for d in docs]\n",
    "        self.doc_lens = [len(t) for t in self.tokenized]\n",
    "        self.avgdl = sum(self.doc_lens)/max(1,len(self.doc_lens))\n",
    "        df = defaultdict(int)\n",
    "        for toks in self.tokenized:\n",
    "            for w in set(toks):\n",
    "                df[w]+=1\n",
    "        self.N = len(docs)\n",
    "        self.idf = {w: math.log(1+(self.N-c+0.5)/(c+0.5)) for w,c in df.items()}\n",
    "        self.tf = [Counter(toks) for toks in self.tokenized]\n",
    "\n",
    "    def _score(self, q_toks, idx):\n",
    "        score=0.0; dl=self.doc_lens[idx]; tf_d=self.tf[idx]\n",
    "        for w in q_toks:\n",
    "            if w not in self.idf: continue\n",
    "            idf=self.idf[w]; f=tf_d.get(w,0)\n",
    "            denom=f+self.k1*(1-self.b+self.b*dl/(self.avgdl or 1))\n",
    "            score+=idf*(f*(self.k1+1))/(denom or 1e-12)\n",
    "        return score\n",
    "\n",
    "    def search(self, query:str,k=5):\n",
    "        q_toks=self.tokenizer(query.lower())\n",
    "        scores=[(i,self._score(q_toks,i)) for i in range(self.N)]\n",
    "        scores.sort(key=lambda x:x[1], reverse=True)\n",
    "        return scores[:k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5873b7",
   "metadata": {},
   "source": [
    "## 3. BioLLM Backend (swappable with other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14d0f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class BioLLMBackend:\n",
    "    def __init__(self, model, tokenizer, device=\"cuda\"):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def generate(self, messages: List[Message]) -> str:\n",
    "        query = next((m.content for m in messages[::-1] if m.role == \"user\"), \"\")\n",
    "        context = \"\\n\\n\".join(m.content for m in messages if m.role in (\"system\", \"tool\"))\n",
    "        context = context[:2000]\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        raw = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = raw.split(\"Answer:\")[-1].strip() \n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a7334",
   "metadata": {},
   "source": [
    "## 4. RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c19db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, docs_df: pd.DataFrame, retriever=\"tfidf\", llm=None):\n",
    "        self.df = docs_df.reset_index(drop=True)\n",
    "        self.contexts = self.df[\"context\"].astype(str).tolist()\n",
    "        if retriever==\"tfidf\":\n",
    "            self.retriever = TFIDFRetriever(self.contexts)\n",
    "        else:\n",
    "            self.retriever = BM25Retriever(self.contexts)\n",
    "        self.llm = llm\n",
    "\n",
    "    def ask(self, query: str, k=3):\n",
    "        hits = self.retriever.search(query, k)\n",
    "        msgs=[Message(role=\"tool\", content=self.contexts[i]) for i,_ in hits]\n",
    "        msgs.append(Message(role=\"user\", content=query))\n",
    "        ans = self.llm.generate(msgs)\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"context\": \" \".join(self.contexts[i][:500] for i,_ in hits),  \n",
    "            \"answer\": ans,\n",
    "            \"hits\": hits\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cb469",
   "metadata": {},
   "source": [
    "## 5. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcabca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "llm = BioLLMBackend(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba162a",
   "metadata": {},
   "source": [
    "## 6. Compare Outputs Across Retrievers / Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8d701de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>retriever</th>\n",
       "      <th>model</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is pathology used in diagnosing soft tissu...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>biollm</td>\n",
       "      <td>Soft tissue sarcomas are a complex group of tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is pathology used in diagnosing soft tissu...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>biollm</td>\n",
       "      <td>In this review, we have discussed the patholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the importance of margins in pathology...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>biollm</td>\n",
       "      <td>The authors of this paper are the first to dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the importance of margins in pathology...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>biollm</td>\n",
       "      <td>Total mastectomy is the best treatment for bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Describe fluorescence in situ hybridization (F...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>biollm</td>\n",
       "      <td>'How can we identify and quantify DNA and RNA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>What is the role of a pathologist in cancer di...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>biollm</td>\n",
       "      <td>A pathologist should be involved in making the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>What are the differences between ductal and lo...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>biollm</td>\n",
       "      <td>The biological markers for invasive breast car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>What are the differences between ductal and lo...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>biollm</td>\n",
       "      <td>The breast carcinoma is a disease of the femal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>What is the difference between sarcomas, carci...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>biollm</td>\n",
       "      <td>The distinction between the two groups of epit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What is the difference between sarcomas, carci...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>biollm</td>\n",
       "      <td>In this article, we will discuss the various h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question retriever   model  \\\n",
       "0    How is pathology used in diagnosing soft tissu...     tfidf  biollm   \n",
       "1    How is pathology used in diagnosing soft tissu...      bm25  biollm   \n",
       "2    What is the importance of margins in pathology...     tfidf  biollm   \n",
       "3    What is the importance of margins in pathology...      bm25  biollm   \n",
       "4    Describe fluorescence in situ hybridization (F...     tfidf  biollm   \n",
       "..                                                 ...       ...     ...   \n",
       "105  What is the role of a pathologist in cancer di...      bm25  biollm   \n",
       "106  What are the differences between ductal and lo...     tfidf  biollm   \n",
       "107  What are the differences between ductal and lo...      bm25  biollm   \n",
       "108  What is the difference between sarcomas, carci...     tfidf  biollm   \n",
       "109  What is the difference between sarcomas, carci...      bm25  biollm   \n",
       "\n",
       "                                                answer  \n",
       "0    Soft tissue sarcomas are a complex group of tu...  \n",
       "1    In this review, we have discussed the patholog...  \n",
       "2    The authors of this paper are the first to dis...  \n",
       "3    Total mastectomy is the best treatment for bre...  \n",
       "4    'How can we identify and quantify DNA and RNA ...  \n",
       "..                                                 ...  \n",
       "105  A pathologist should be involved in making the...  \n",
       "106  The biological markers for invasive breast car...  \n",
       "107  The breast carcinoma is a disease of the femal...  \n",
       "108  The distinction between the two groups of epit...  \n",
       "109  In this article, we will discuss the various h...  \n",
       "\n",
       "[110 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def compare_answers(df_qa: pd.DataFrame, retrievers=[\"tfidf\",\"bm25\"], llms=[(\"biogpt\", llm)], n=5):\n",
    "    sample = df_qa.sample(min(n, len(df_qa)), random_state=0)\n",
    "    rows=[]\n",
    "    for _,row in sample.iterrows():\n",
    "        q = str(row[\"question\"])\n",
    "        for rname in retrievers:\n",
    "            for lname, lbackend in llms:\n",
    "                rag = SimpleRAG(docs_df, retriever=rname, llm=lbackend)\n",
    "                out = rag.ask(q, k=3)\n",
    "                rows.append({\"question\":q,\"retriever\":rname,\"model\":lname,\"answer\":out[\"answer\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "results = compare_answers(df_qa, retrievers=[\"tfidf\",\"bm25\"], llms=[(\"biollm\", llm)], n=100)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e22fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"rag_results.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd05ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb5418fa",
   "metadata": {},
   "source": [
    "## Added: Embedding-based retrieval & comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0881d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Embedding backends config ===\n",
    "EMBED_MODELS = [\n",
    "    (\"minilm\", \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    (\"bge-small\", \"BAAI/bge-small-en-v1.5\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "652dbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingBackend:\n",
    "    def embed_texts(self, texts):\n",
    "        raise NotImplementedError\n",
    "    def embed_query(self, text):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SentenceTransformersEmbedding(EmbeddingBackend):\n",
    "    def __init__(self, model_id: str, device: str = \"cuda\"):\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"sentence-transformers not installed. pip install sentence-transformers\") from e\n",
    "        self.model = SentenceTransformer(model_id, device=device)\n",
    "\n",
    "    def embed_texts(self, texts):\n",
    "        vecs = self.model.encode(texts, batch_size=64, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        return vecs\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_texts([text])[0]\n",
    "\n",
    "class HFMeanPoolingEmbedding(EmbeddingBackend):\n",
    "    def __init__(self, model_id: str, device: str = \"cuda\"):\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModel.from_pretrained(model_id).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def _mean_pool(self, outputs, attention_mask):\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "        masked = last_hidden * mask\n",
    "        summed = masked.sum(1)\n",
    "        counts = mask.sum(1).clamp(min=1e-9)\n",
    "        return (summed / counts).detach().cpu().numpy()\n",
    "\n",
    "    def embed_texts(self, texts):\n",
    "        import torch, numpy as _np\n",
    "        all_vecs = []\n",
    "        bs = 16\n",
    "        for i in range(0, len(texts), bs):\n",
    "            batch = texts[i:i+bs]\n",
    "            enc = self.tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.model(**enc)\n",
    "            vecs = self._mean_pool(out, enc[\"attention_mask\"])\n",
    "            vecs = vecs / (_np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-9)\n",
    "            all_vecs.append(vecs)\n",
    "        return _np.vstack(all_vecs)\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_texts([text])[0]\n",
    "\n",
    "class EmbeddingRetriever:\n",
    "    def __init__(self, docs, backend: EmbeddingBackend):\n",
    "        self.docs = docs\n",
    "        self.backend = backend\n",
    "        self.doc_vecs = self.backend.embed_texts(docs)\n",
    "\n",
    "    def search(self, query: str, k=5):\n",
    "        q = self.backend.embed_query(query)\n",
    "        sims = (self.doc_vecs @ q)\n",
    "        idxs = np.argsort(-sims)[:k]\n",
    "        return [(int(i), float(sims[i])) for i in idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dad5c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, docs_df: pd.DataFrame, retriever=\"tfidf\", llm=None, embed_backend: EmbeddingBackend = None):\n",
    "        self.df = docs_df.reset_index(drop=True)\n",
    "        self.contexts = self.df[\"context\"].astype(str).tolist()\n",
    "        self.llm = llm\n",
    "\n",
    "        if retriever == \"tfidf\":\n",
    "            self.retriever = TFIDFRetriever(self.contexts)\n",
    "            self.retriever_name = \"tfidf\"\n",
    "            self.embedding_name = \"-\"\n",
    "        elif retriever == \"bm25\":\n",
    "            self.retriever = BM25Retriever(self.contexts)\n",
    "            self.retriever_name = \"bm25\"\n",
    "            self.embedding_name = \"-\"\n",
    "        elif retriever == \"embed\":\n",
    "            if embed_backend is None:\n",
    "                raise ValueError(\"embed_backend must be provided when retriever='embed'\")\n",
    "            self.retriever = EmbeddingRetriever(self.contexts, embed_backend)\n",
    "            self.retriever_name = \"embed\"\n",
    "            self.embedding_name = getattr(embed_backend, \"model\", getattr(embed_backend, \"__class__\", type(embed_backend))).__class__.__name__\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown retriever: {retriever}\")\n",
    "\n",
    "    def ask(self, query: str, k=3):\n",
    "        hits = self.retriever.search(query, k)\n",
    "        msgs = [Message(role=\"tool\", content=self.contexts[i][:2000]) for i,_ in hits]\n",
    "        msgs.append(Message(role=\"user\", content=query))\n",
    "        ans = self.llm.generate(msgs)\n",
    "        combined_ctx = \" \".join(self.contexts[i][:500] for i,_ in hits)\n",
    "        return {\"query\":query, \"answer\":ans, \"hits\":hits, \"context\": combined_ctx}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e5365a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_answers(df_qa: pd.DataFrame, retrievers, llms, embed_models=None, n=5, device=\"cuda\"):\n",
    "    sample = df_qa.sample(min(n, len(df_qa)), random_state=0)\n",
    "    rows = []\n",
    "\n",
    "    def build_backend(model_id: str):\n",
    "        try:\n",
    "            return SentenceTransformersEmbedding(model_id, device=device)\n",
    "        except Exception:\n",
    "            return HFMeanPoolingEmbedding(model_id, device=device)\n",
    "\n",
    "    for _, row in sample.iterrows():\n",
    "        q = str(row[\"question\"])\n",
    "        for lname, lbackend in llms:\n",
    "            for rname in retrievers:\n",
    "                if rname == \"embed\":\n",
    "                    if not embed_models:\n",
    "                        continue\n",
    "                    for embed_short, embed_id in embed_models:\n",
    "                        backend = build_backend(embed_id)\n",
    "                        rag = SimpleRAG(docs_df, retriever=\"embed\", llm=lbackend, embed_backend=backend)\n",
    "                        out = rag.ask(q, k=3)\n",
    "                        rows.append({\n",
    "                            \"question\": q,\n",
    "                            \"retriever\": \"embed\",\n",
    "                            \"embedding\": embed_short,\n",
    "                            \"model\": lname,\n",
    "                            \"answer\": out[\"answer\"],\n",
    "                            \"context\": out[\"context\"]\n",
    "                        })\n",
    "                else:\n",
    "                    rag = SimpleRAG(docs_df, retriever=rname, llm=lbackend)\n",
    "                    out = rag.ask(q, k=3)\n",
    "                    rows.append({\n",
    "                        \"question\": q,\n",
    "                        \"retriever\": rname,\n",
    "                        \"embedding\": \"-\",\n",
    "                        \"model\": lname,\n",
    "                        \"answer\": out[\"answer\"],\n",
    "                        \"context\": out[\"context\"]\n",
    "                    })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bbdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> rag_results_with_embeddings.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare across TF-IDF, BM25, and Embedding retrievers; include embedding column\n",
    "results = compare_answers(\n",
    "    df_qa,\n",
    "    retrievers=[\"tfidf\", \"bm25\", \"embed\"],\n",
    "    llms=[(\"biollm\", llm)],\n",
    "    embed_models=EMBED_MODELS,\n",
    "    n=5\n",
    ")\n",
    "results\n",
    "\n",
    "# Save\n",
    "results.to_csv(\"rag_results_with_embeddings.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Saved -> rag_results_with_embeddings.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ultrasound-3.11)",
   "language": "python",
   "name": "ultrasound311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
